<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-04-09T17:31:00+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Carmen Amo Alonso</title><subtitle>Carmen Amo Alonso</subtitle><entry><title type="html">Global performance guarantees for localized model predictive control</title><link href="http://localhost:4000/publications/2023/09/01/DLMPC-Global-Optimality.html" rel="alternate" type="text/html" title="Global performance guarantees for localized model predictive control" /><published>2023-09-01T00:00:00+02:00</published><updated>2023-09-01T00:00:00+02:00</updated><id>http://localhost:4000/publications/2023/09/01/DLMPC-Global-Optimality</id><content type="html" xml:base="http://localhost:4000/publications/2023/09/01/DLMPC-Global-Optimality.html">&lt;p&gt;Recent advances in model predictive control (MPC) leverage local communication constraints to produce localized MPC algorithms whose complexities scale independently of total network size. However, no characterization is available regarding global performance, i.e. whether localized MPC (with communication constraints) performs just as well as global MPC (no communication constraints). In this paper, we provide analysis and guarantees on global performance of localized MPC – in particular, we derive sufficient conditions for optimal global performance in the presence of local communication constraints. We also present an algorithm to determine the communication structure for a given system that will preserve performance while minimizing computational complexity. The effectiveness of the algorithm is verified in simulations, and additional relationships between network properties and performance-preserving communication constraints are characterized. A striking finding is that in a network of 121 coupled pendula, each node only needs to communicate with its immediate neighbors to preserve optimal global performance. Overall, this work offers theoretical understanding on the effect of local communication on global performance, and provides practitioners with the tools necessary to deploy localized model predictive control by establishing a rigorous method of selecting local communication constraints. This work also demonstrates – surprisingly – that the inclusion of severe communication constraints need not compromise global performance.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">Recent advances in model predictive control (MPC) leverage local communication constraints to produce localized MPC algorithms whose complexities scale independently of total network size. However, no characterization is available regarding global performance, i.e. whether localized MPC (with communication constraints) performs just as well as global MPC (no communication constraints). In this paper, we provide analysis and guarantees on global performance of localized MPC – in particular, we derive sufficient conditions for optimal global performance in the presence of local communication constraints. We also present an algorithm to determine the communication structure for a given system that will preserve performance while minimizing computational complexity. The effectiveness of the algorithm is verified in simulations, and additional relationships between network properties and performance-preserving communication constraints are characterized. A striking finding is that in a network of 121 coupled pendula, each node only needs to communicate with its immediate neighbors to preserve optimal global performance. Overall, this work offers theoretical understanding on the effect of local communication on global performance, and provides practitioners with the tools necessary to deploy localized model predictive control by establishing a rigorous method of selecting local communication constraints. This work also demonstrates – surprisingly – that the inclusion of severe communication constraints need not compromise global performance.</summary></entry><entry><title type="html">Distributed and Localized Model Predictive Control</title><link href="http://localhost:4000/publications/2023/06/01/DLMPC-thesis.html" rel="alternate" type="text/html" title="Distributed and Localized Model Predictive Control" /><published>2023-06-01T00:00:00+02:00</published><updated>2023-06-01T00:00:00+02:00</updated><id>http://localhost:4000/publications/2023/06/01/DLMPC-thesis</id><content type="html" xml:base="http://localhost:4000/publications/2023/06/01/DLMPC-thesis.html">&lt;p&gt;The increasing presence of large-scale distributed systems highlights the need for scalable control strategies where only local communication is required. Moreover, in safety-critical systems it is imperative that such control strategies handle constraints in the presence of disturbances and enjoy theoretical and performance guarantees. In response to this need, we present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale linear systems. DLMPC is a distributed closed-loop model predictive control (MPC) scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. The resulting distributed algorithms tackle various types of additive disturbances and enjoy recursive feasibility and asymptotic stability guarantees that introduce minimal conservatism and can be computed in an offline fashion without adding to the computational burden. We also provide analysis and guarantees on the global performance of DLMPC, and demonstrate that in cases where the underlying topology of the system is sparse (as is the case in most large-scale networks), the inclusion of local communication constraints does not result in a suboptimal solution. Moreover, we show that when no noise is present, this algorithm can be extended to the purely data-driven case where all previous guarantees hold and the need for a model is fully replaced by past-trajectory data. We show that the amount of data needed for our synthesis problem is independent of the size of the global system. Lastly, we explore the potential of DLMPC for hardware accelerated implementation in GPU by exploiting the fact that the structure of the DLMPC problem captures some of the limitations of GPU computations. In all algorithmic and theoretical results presented in this thesis, only local information exchange is necessary, and computational complexity is independent of the global system size. DLMPC is the first MPC algorithm that allows for the scalable, efficient and data-driven computation and implementation of distributed closed-loop control policies and enjoys theoretical guarantees.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">The increasing presence of large-scale distributed systems highlights the need for scalable control strategies where only local communication is required. Moreover, in safety-critical systems it is imperative that such control strategies handle constraints in the presence of disturbances and enjoy theoretical and performance guarantees. In response to this need, we present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale linear systems. DLMPC is a distributed closed-loop model predictive control (MPC) scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. The resulting distributed algorithms tackle various types of additive disturbances and enjoy recursive feasibility and asymptotic stability guarantees that introduce minimal conservatism and can be computed in an offline fashion without adding to the computational burden. We also provide analysis and guarantees on the global performance of DLMPC, and demonstrate that in cases where the underlying topology of the system is sparse (as is the case in most large-scale networks), the inclusion of local communication constraints does not result in a suboptimal solution. Moreover, we show that when no noise is present, this algorithm can be extended to the purely data-driven case where all previous guarantees hold and the need for a model is fully replaced by past-trajectory data. We show that the amount of data needed for our synthesis problem is independent of the size of the global system. Lastly, we explore the potential of DLMPC for hardware accelerated implementation in GPU by exploiting the fact that the structure of the DLMPC problem captures some of the limitations of GPU computations. In all algorithmic and theoretical results presented in this thesis, only local information exchange is necessary, and computational complexity is independent of the global system size. DLMPC is the first MPC algorithm that allows for the scalable, efficient and data-driven computation and implementation of distributed closed-loop control policies and enjoys theoretical guarantees.</summary></entry><entry><title type="html">Distributed and Localized Model Predictive Control. Part II: Theoretical Guarantees</title><link href="http://localhost:4000/publications/2023/03/01/DLMPC-Part-II.html" rel="alternate" type="text/html" title="Distributed and Localized Model Predictive Control. Part II: Theoretical Guarantees" /><published>2023-03-01T00:00:00+01:00</published><updated>2023-03-01T00:00:00+01:00</updated><id>http://localhost:4000/publications/2023/03/01/DLMPC-Part-II</id><content type="html" xml:base="http://localhost:4000/publications/2023/03/01/DLMPC-Part-II.html">&lt;p&gt;Engineered cyberphysical systems are growing increasingly large and complex. These systems require scalable controllers that robustly satisfy state and input constraints in the presence of additive noise – such controllers should also be accompanied by theoretical guarantees on feasibility and stability. In our companion paper, we introduced Distributed and Localized Model Predictive Control (DLMPC) for large-scale linear systems; DLMPC is a scalable closed-loop MPC scheme in which subsystems need only exchange local information in order to synthesize and implement local controllers. In this paper, we provide recursive feasibility and asymptotic stability guarantees for DLMPC. We leverage the System Level Synthesis framework to express the maximal positive robust invariant set for the closed-loop system and its corresponding Lyapunov function, both in terms of the closed-loop system responses. We use the invariant set as the terminal set for DLMPC, and show that this guarantees feasibility with minimal conservatism. We use the Lyapunov function as the terminal cost, and show that this guarantees stability. We provide fully distributed and localized algorithms to compute the terminal set offline, and also provide necessary additions to the online DLMPC algorithm to accommodate coupled terminal constraint and cost. In all algorithms, only local information exchanges are necessary, and computational complexity is independent of the global system size – we demonstrate this analytically and experimentally. This is the first distributed MPC approach that provides minimally conservative yet fully distributed guarantees for recursive feasibility and asymptotic stability, for both nominal and robust settings.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">Engineered cyberphysical systems are growing increasingly large and complex. These systems require scalable controllers that robustly satisfy state and input constraints in the presence of additive noise – such controllers should also be accompanied by theoretical guarantees on feasibility and stability. In our companion paper, we introduced Distributed and Localized Model Predictive Control (DLMPC) for large-scale linear systems; DLMPC is a scalable closed-loop MPC scheme in which subsystems need only exchange local information in order to synthesize and implement local controllers. In this paper, we provide recursive feasibility and asymptotic stability guarantees for DLMPC. We leverage the System Level Synthesis framework to express the maximal positive robust invariant set for the closed-loop system and its corresponding Lyapunov function, both in terms of the closed-loop system responses. We use the invariant set as the terminal set for DLMPC, and show that this guarantees feasibility with minimal conservatism. We use the Lyapunov function as the terminal cost, and show that this guarantees stability. We provide fully distributed and localized algorithms to compute the terminal set offline, and also provide necessary additions to the online DLMPC algorithm to accommodate coupled terminal constraint and cost. In all algorithms, only local information exchanges are necessary, and computational complexity is independent of the global system size – we demonstrate this analytically and experimentally. This is the first distributed MPC approach that provides minimally conservative yet fully distributed guarantees for recursive feasibility and asymptotic stability, for both nominal and robust settings.</summary></entry><entry><title type="html">Distributed and Localized Model Predictive Control. Part I: Synthesis and Implementation</title><link href="http://localhost:4000/publications/2023/03/01/DLMPC-Part-I.html" rel="alternate" type="text/html" title="Distributed and Localized Model Predictive Control. Part I: Synthesis and Implementation" /><published>2023-03-01T00:00:00+01:00</published><updated>2023-03-01T00:00:00+01:00</updated><id>http://localhost:4000/publications/2023/03/01/DLMPC-Part-I</id><content type="html" xml:base="http://localhost:4000/publications/2023/03/01/DLMPC-Part-I.html">&lt;p&gt;The increasing presence of large-scale distributed systems highlights the need for scalable control strategies where only local communication is required. Moreover, in safety-critical systems it is imperative that such control strategies handle constraints in the presence of disturbances. In response to this need, we present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale linear systems. DLMPC is a distributed closed-loop model predictive control (MPC) scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We use the System Level Synthesis (SLS) framework to reformulate the centralized MPC problem, and show that this allows us to naturally impose localized communication constraints between sub-controllers. The structure of the resulting problem can be exploited to develop an Alternating Direction Method of Multipliers (ADMM) based algorithm that allows for distributed and localized computation of closed-loop control policies. We demonstrate that computational complexity of the subproblems solved by each subsystem in DLMPC is independent of the size of the global system. To the best of our knowledge, DLMPC is the first MPC algorithm that allows for the scalable distributed computation as well as implementation of distributed closed-loop control policies, and seemingly deals with additive disturbances. In our companion paper, we show that this approach enjoys recursive feasibility and asymptotic stability.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">The increasing presence of large-scale distributed systems highlights the need for scalable control strategies where only local communication is required. Moreover, in safety-critical systems it is imperative that such control strategies handle constraints in the presence of disturbances. In response to this need, we present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale linear systems. DLMPC is a distributed closed-loop model predictive control (MPC) scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We use the System Level Synthesis (SLS) framework to reformulate the centralized MPC problem, and show that this allows us to naturally impose localized communication constraints between sub-controllers. The structure of the resulting problem can be exploited to develop an Alternating Direction Method of Multipliers (ADMM) based algorithm that allows for distributed and localized computation of closed-loop control policies. We demonstrate that computational complexity of the subproblems solved by each subsystem in DLMPC is independent of the size of the global system. To the best of our knowledge, DLMPC is the first MPC algorithm that allows for the scalable distributed computation as well as implementation of distributed closed-loop control policies, and seemingly deals with additive disturbances. In our companion paper, we show that this approach enjoys recursive feasibility and asymptotic stability.</summary></entry><entry><title type="html">Effective GPU Parallelization of Distributed and Localized Model Predictive Control</title><link href="http://localhost:4000/publications/2022/06/01/DLMPC-GPU.html" rel="alternate" type="text/html" title="Effective GPU Parallelization of Distributed and Localized Model Predictive Control" /><published>2022-06-01T00:00:00+02:00</published><updated>2022-06-01T00:00:00+02:00</updated><id>http://localhost:4000/publications/2022/06/01/DLMPC-GPU</id><content type="html" xml:base="http://localhost:4000/publications/2022/06/01/DLMPC-GPU.html">&lt;p&gt;To effectively control large-scale distributed systems online, model predictive control (MPC) has to swiftly solve the underlying high-dimensional optimization. There are multiple techniques applied to accelerate the solving process in the literature, mainly attributed to software-based algorithmic advancements and hardware-assisted computation enhancements. However, those methods focus on arithmetic accelerations and overlook the benefits of the underlying system’s structure. In particular, the existing decoupled software-hardware algorithm design that naively parallelizes the arithmetic operations by the hardware does not tackle the hardware overheads such as CPU-GPU and thread-to-thread communications in a principled manner. Also, the advantages of parallelizable subproblem decomposition in distributed MPC are not well recognized and exploited. As a result, we have not reached the full potential of hardware acceleration for MPC. In this paper, we explore those opportunities by leveraging GPU to parallelize the distributed and localized MPC (DLMPC) algorithm. We exploit the locality constraints embedded in the DLMPC formulation to reduce the hardware-intrinsic communication overheads. Our parallel implementation achieves up to 50x faster runtime than its CPU counterparts under various parameters. Furthermore, we find that the locality-aware GPU parallelization could halve the optimization runtime comparing to the naive acceleration. Overall, our results demonstrate the performance gains brought by software-hardware co-design with the information exchange structure in mind.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">To effectively control large-scale distributed systems online, model predictive control (MPC) has to swiftly solve the underlying high-dimensional optimization. There are multiple techniques applied to accelerate the solving process in the literature, mainly attributed to software-based algorithmic advancements and hardware-assisted computation enhancements. However, those methods focus on arithmetic accelerations and overlook the benefits of the underlying system’s structure. In particular, the existing decoupled software-hardware algorithm design that naively parallelizes the arithmetic operations by the hardware does not tackle the hardware overheads such as CPU-GPU and thread-to-thread communications in a principled manner. Also, the advantages of parallelizable subproblem decomposition in distributed MPC are not well recognized and exploited. As a result, we have not reached the full potential of hardware acceleration for MPC. In this paper, we explore those opportunities by leveraging GPU to parallelize the distributed and localized MPC (DLMPC) algorithm. We exploit the locality constraints embedded in the DLMPC formulation to reduce the hardware-intrinsic communication overheads. Our parallel implementation achieves up to 50x faster runtime than its CPU counterparts under various parameters. Furthermore, we find that the locality-aware GPU parallelization could halve the optimization runtime comparing to the naive acceleration. Overall, our results demonstrate the performance gains brought by software-hardware co-design with the information exchange structure in mind.</summary></entry><entry><title type="html">Data-driven Distributed and Localized Model Predictive Control</title><link href="http://localhost:4000/publications/2022/04/01/D3LMPC.html" rel="alternate" type="text/html" title="Data-driven Distributed and Localized Model Predictive Control" /><published>2022-04-01T00:00:00+02:00</published><updated>2022-04-01T00:00:00+02:00</updated><id>http://localhost:4000/publications/2022/04/01/D3LMPC</id><content type="html" xml:base="http://localhost:4000/publications/2022/04/01/D3LMPC.html">&lt;p&gt;Motivated by large-scale but computationally constrained settings, e.g., the Internet of Things, we present a novel data-driven distributed control algorithm that is synthesized directly from trajectory data. Our method, data-driven Distributed and Localized Model Predictive Control (D3LMPC), builds upon the data-driven System Level Synthesis (SLS) framework, which allows one to parameterize \emph{closed-loop} system responses directly from collected open-loop trajectories. The resulting model-predictive controller can be implemented with distributed computation and only local information sharing. By imposing locality constraints on the system response, we show that the amount of data needed for our synthesis problem is independent of the size of the global system. Moreover, we show that our algorithm enjoys theoretical guarantees for recursive feasibility and asymptotic stability. Finally, we also demonstrate the optimality and scalability of our algorithm in a simulation experiment.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">Motivated by large-scale but computationally constrained settings, e.g., the Internet of Things, we present a novel data-driven distributed control algorithm that is synthesized directly from trajectory data. Our method, data-driven Distributed and Localized Model Predictive Control (D3LMPC), builds upon the data-driven System Level Synthesis (SLS) framework, which allows one to parameterize \emph{closed-loop} system responses directly from collected open-loop trajectories. The resulting model-predictive controller can be implemented with distributed computation and only local information sharing. By imposing locality constraints on the system response, we show that the amount of data needed for our synthesis problem is independent of the size of the global system. Moreover, we show that our algorithm enjoys theoretical guarantees for recursive feasibility and asymptotic stability. Finally, we also demonstrate the optimality and scalability of our algorithm in a simulation experiment.</summary></entry><entry><title type="html">Frontiers in Scalable Distributed Control: SLS, MPC, and Beyond</title><link href="http://localhost:4000/publications/2021/06/01/Frontiers.html" rel="alternate" type="text/html" title="Frontiers in Scalable Distributed Control: SLS, MPC, and Beyond" /><published>2021-06-01T00:00:00+02:00</published><updated>2021-06-01T00:00:00+02:00</updated><id>http://localhost:4000/publications/2021/06/01/Frontiers</id><content type="html" xml:base="http://localhost:4000/publications/2021/06/01/Frontiers.html">&lt;p&gt;The System Level Synthesis (SLS) approach facilitates distributed control of large cyberphysical networks in an easy-to-understand, computationally scalable way. We present an overview of the SLS approach and its associated extensions in nonlinear control, MPC, adaptive control, and learning for control. To illustrate the effectiveness of SLS-based methods, we present a case study motivated by the power grid, with communication constraints, actuator saturation, disturbances, and changing setpoints. This simple but challenging case study necessitates the use of model predictive control (MPC); however, standard MPC techniques often scales poorly to large systems and incurs heavy computational burden. To address this challenge, we combine two SLS-based controllers to form a layered MPC-like controller. Our controller has constant computational complexity with respect to the system size, gives a 20-fold reduction in online computation requirements, and still achieves performance that is within 3% of the centralized MPC controller.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">The System Level Synthesis (SLS) approach facilitates distributed control of large cyberphysical networks in an easy-to-understand, computationally scalable way. We present an overview of the SLS approach and its associated extensions in nonlinear control, MPC, adaptive control, and learning for control. To illustrate the effectiveness of SLS-based methods, we present a case study motivated by the power grid, with communication constraints, actuator saturation, disturbances, and changing setpoints. This simple but challenging case study necessitates the use of model predictive control (MPC); however, standard MPC techniques often scales poorly to large systems and incurs heavy computational burden. To address this challenge, we combine two SLS-based controllers to form a layered MPC-like controller. Our controller has constant computational complexity with respect to the system size, gives a 20-fold reduction in online computation requirements, and still achieves performance that is within 3% of the centralized MPC controller.</summary></entry><entry><title type="html">Distributed and Localized Model Predictive Control via System Level Synthesis</title><link href="http://localhost:4000/publications/2020/12/02/DLMPC.html" rel="alternate" type="text/html" title="Distributed and Localized Model Predictive Control via System Level Synthesis" /><published>2020-12-02T00:00:00+01:00</published><updated>2020-12-02T00:00:00+01:00</updated><id>http://localhost:4000/publications/2020/12/02/DLMPC</id><content type="html" xml:base="http://localhost:4000/publications/2020/12/02/DLMPC.html">&lt;p&gt;We present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale structured linear systems, a distributed closed loop model predictive control scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We use the System Level Synthesis (SLS) framework to reformulate the centralized MPC problem as an optimization problem over closed loop system responses, and show that this allows us to naturally impose localized communication constraints between sub-controllers. We show that the structure of the resulting optimization problem can be exploited to develop an Alternating Direction Method of Multipliers (ADMM) based algorithm that allows for distributed and localized computation of distributed closed loop control policies. We conclude with numerical simulations to demonstrate the usefulness of our method, in which we show that the computational complexity of the subproblems solved by each subsystem in DLMPC is independent of the size of the global system. To the best of our knowledge, DLMPC is the first MPC algorithm that allows for the scalable distributed computation of distributed closed loop control policies.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">We present the Distributed and Localized Model Predictive Control (DLMPC) algorithm for large-scale structured linear systems, a distributed closed loop model predictive control scheme wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We use the System Level Synthesis (SLS) framework to reformulate the centralized MPC problem as an optimization problem over closed loop system responses, and show that this allows us to naturally impose localized communication constraints between sub-controllers. We show that the structure of the resulting optimization problem can be exploited to develop an Alternating Direction Method of Multipliers (ADMM) based algorithm that allows for distributed and localized computation of distributed closed loop control policies. We conclude with numerical simulations to demonstrate the usefulness of our method, in which we show that the computational complexity of the subproblems solved by each subsystem in DLMPC is independent of the size of the global system. To the best of our knowledge, DLMPC is the first MPC algorithm that allows for the scalable distributed computation of distributed closed loop control policies.</summary></entry><entry><title type="html">Explicit Distributed and Localized Model Predictive Control via System Level Synthesis</title><link href="http://localhost:4000/publications/2020/12/02/eDLMPC.html" rel="alternate" type="text/html" title="Explicit Distributed and Localized Model Predictive Control via System Level Synthesis" /><published>2020-12-02T00:00:00+01:00</published><updated>2020-12-02T00:00:00+01:00</updated><id>http://localhost:4000/publications/2020/12/02/eDLMPC</id><content type="html" xml:base="http://localhost:4000/publications/2020/12/02/eDLMPC.html">&lt;p&gt;An explicit Model Predictive Control algorithm for large-scale structured linear systems is presented. We base our results on Distributed and Localized Model Predictive Control (DLMPC), a closed loop model predictive control scheme based on the System Level Synthesis (SLS) framework wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We provide an explicit solution for each of the subproblems resulting from the distributed MPC scheme. We show that given the separability of the problem, the explicit solution is only divided into three regions per state and input instantiation, making the point location problem very efficient. Moreover, given the locality constraints, the subproblems are of much smaller dimension than the full problem, which consequently significantly reduces the computational overhead of explicit solutions. We conclude with numerical simulations to demonstrate the computational advantages of our method, in which we show a large improvement in runtime per MPC iteration as compared with the results of computing the optimization with a solver online.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">An explicit Model Predictive Control algorithm for large-scale structured linear systems is presented. We base our results on Distributed and Localized Model Predictive Control (DLMPC), a closed loop model predictive control scheme based on the System Level Synthesis (SLS) framework wherein only local state and model information needs to be exchanged between subsystems for the computation and implementation of control actions. We provide an explicit solution for each of the subproblems resulting from the distributed MPC scheme. We show that given the separability of the problem, the explicit solution is only divided into three regions per state and input instantiation, making the point location problem very efficient. Moreover, given the locality constraints, the subproblems are of much smaller dimension than the full problem, which consequently significantly reduces the computational overhead of explicit solutions. We conclude with numerical simulations to demonstrate the computational advantages of our method, in which we show a large improvement in runtime per MPC iteration as compared with the results of computing the optimization with a solver online.</summary></entry><entry><title type="html">System Level Synthesis via Dynamic Programming</title><link href="http://localhost:4000/publications/2020/12/01/SLSDP.html" rel="alternate" type="text/html" title="System Level Synthesis via Dynamic Programming" /><published>2020-12-01T00:00:00+01:00</published><updated>2020-12-01T00:00:00+01:00</updated><id>http://localhost:4000/publications/2020/12/01/SLSDP</id><content type="html" xml:base="http://localhost:4000/publications/2020/12/01/SLSDP.html">&lt;p&gt;System Level Synthesis (SLS) parametrization facilitates controller synthesis for large, complex, and distributed systems by incorporating system level constraints (SLCs) into a convex SLS problem and mapping its solution to stable controller design. Solving the SLS problem at scale efficiently is challenging, and current attempts take advantage of special system or controller structures to speed up the computation in parallel. However, those methods do not generalize as they rely on the specific system/controller properties.We argue that it is possible to solve general SLS problems more efficiently by exploiting the structure of SLS constraints. In particular, we derive dynamic programming (DP) algorithms to solve SLS problems. In addition to the plain SLS without any SLCs, we extend DP to tackle infinite horizon SLS approximation and entrywise linear constraints, which form a superclass of the locality constraints. Comparing to convex program solver and naive analytical derivation, DP solves SLS 4 to 12× faster and scales with little computation overhead. We also quantize the cost of synthesizing a controller that stabilizes the system in a finite horizon through simulations.&lt;/p&gt;</content><author><name></name></author><category term="publications" /><summary type="html">System Level Synthesis (SLS) parametrization facilitates controller synthesis for large, complex, and distributed systems by incorporating system level constraints (SLCs) into a convex SLS problem and mapping its solution to stable controller design. Solving the SLS problem at scale efficiently is challenging, and current attempts take advantage of special system or controller structures to speed up the computation in parallel. However, those methods do not generalize as they rely on the specific system/controller properties.We argue that it is possible to solve general SLS problems more efficiently by exploiting the structure of SLS constraints. In particular, we derive dynamic programming (DP) algorithms to solve SLS problems. In addition to the plain SLS without any SLCs, we extend DP to tackle infinite horizon SLS approximation and entrywise linear constraints, which form a superclass of the locality constraints. Comparing to convex program solver and naive analytical derivation, DP solves SLS 4 to 12× faster and scales with little computation overhead. We also quantize the cost of synthesizing a controller that stabilizes the system in a finite horizon through simulations.</summary></entry></feed>